{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from pycocotools.coco import COCO\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "from tqdm import tqdm\n",
    "pylab.rcParams['figure.figsize'] = (8.0, 10.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import os.path\n",
    "import random\n",
    "import sys\n",
    "import xml.etree.ElementTree\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.data\n",
    "import cv2\n",
    "import PIL.Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from argparse import ArgumentParser\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "from model.networks import Generator\n",
    "from utils.tools import get_config, random_bbox, mask_image, is_image_file, default_loader, normalize, get_model_list\n",
    "\n",
    "SEED = 42 \n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parser = ArgumentParser()\n",
    "parser.add_argument('--config', type=str, default='configs/config.yaml',\n",
    "                    help=\"training configuration\")\n",
    "parser.add_argument('--seed', type=int, help='manual seed')\n",
    "parser.add_argument('--image', type=str)\n",
    "parser.add_argument('--mask', type=str, default='')\n",
    "parser.add_argument('--output', type=str, default='output.png')\n",
    "parser.add_argument('--flow', type=str, default='')\n",
    "parser.add_argument('--checkpoint_path', type=str, default='')\n",
    "parser.add_argument('--iter', type=int, default=0)\n",
    "\n",
    "python test_single.py \\\n",
    "\t--image examples/imagenet/imagenet_patches_ILSVRC2012_val_00008210_input.png \\\n",
    "\t--mask examples/center_mask_256.png \\\n",
    "\t--output examples/output.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COCO Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir='datasets/coco'\n",
    "dataType='val2017'\n",
    "annFile_full='{}/annotations/instances_{}.json'.format(dataDir,dataType)\n",
    "annFile_human_pose = '{}/annotations/person_keypoints_{}.json'.format(dataDir,dataType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "irange = range\n",
    "\n",
    "\n",
    "def make_grid(tensor, nrow=8, padding=2,\n",
    "              normalize=False, range=None, scale_each=False, pad_value=0):\n",
    "    \"\"\"Make a grid of images.\n",
    "\n",
    "    Args:\n",
    "        tensor (Tensor or list): 4D mini-batch Tensor of shape (B x C x H x W)\n",
    "            or a list of images all of the same size.\n",
    "        nrow (int, optional): Number of images displayed in each row of the grid.\n",
    "            The final grid size is ``(B / nrow, nrow)``. Default: ``8``.\n",
    "        padding (int, optional): amount of padding. Default: ``2``.\n",
    "        normalize (bool, optional): If True, shift the image to the range (0, 1),\n",
    "            by the min and max values specified by :attr:`range`. Default: ``False``.\n",
    "        range (tuple, optional): tuple (min, max) where min and max are numbers,\n",
    "            then these numbers are used to normalize the image. By default, min and max\n",
    "            are computed from the tensor.\n",
    "        scale_each (bool, optional): If ``True``, scale each image in the batch of\n",
    "            images separately rather than the (min, max) over all images. Default: ``False``.\n",
    "        pad_value (float, optional): Value for the padded pixels. Default: ``0``.\n",
    "\n",
    "    Example:\n",
    "        See this notebook `here <https://gist.github.com/anonymous/bf16430f7750c023141c562f3e9f2a91>`_\n",
    "\n",
    "    \"\"\"\n",
    "    if not (torch.is_tensor(tensor) or\n",
    "            (isinstance(tensor, list) and all(torch.is_tensor(t) for t in tensor))):\n",
    "        raise TypeError('tensor or list of tensors expected, got {}'.format(type(tensor)))\n",
    "\n",
    "    # if list of tensors, convert to a 4D mini-batch Tensor\n",
    "    if isinstance(tensor, list):\n",
    "        tensor = torch.stack(tensor, dim=0)\n",
    "\n",
    "    if tensor.dim() == 2:  # single image H x W\n",
    "        tensor = tensor.unsqueeze(0)\n",
    "    if tensor.dim() == 3:  # single image\n",
    "        if tensor.size(0) == 1:  # if single-channel, convert to 3-channel\n",
    "            tensor = torch.cat((tensor, tensor, tensor), 0)\n",
    "        tensor = tensor.unsqueeze(0)\n",
    "\n",
    "    if tensor.dim() == 4 and tensor.size(1) == 1:  # single-channel images\n",
    "        tensor = torch.cat((tensor, tensor, tensor), 1)\n",
    "\n",
    "    if normalize is True:\n",
    "        tensor = tensor.clone()  # avoid modifying tensor in-place\n",
    "        if range is not None:\n",
    "            assert isinstance(range, tuple), \\\n",
    "                \"range has to be a tuple (min, max) if specified. min and max are numbers\"\n",
    "\n",
    "        def norm_ip(img, min, max):\n",
    "            img.clamp_(min=min, max=max)\n",
    "            img.add_(-min).div_(max - min + 1e-5)\n",
    "\n",
    "        def norm_range(t, range):\n",
    "            if range is not None:\n",
    "                norm_ip(t, range[0], range[1])\n",
    "            else:\n",
    "                norm_ip(t, float(t.min()), float(t.max()))\n",
    "\n",
    "        if scale_each is True:\n",
    "            for t in tensor:  # loop over mini-batch dimension\n",
    "                norm_range(t, range)\n",
    "        else:\n",
    "            norm_range(tensor, range)\n",
    "\n",
    "    if tensor.size(0) == 1:\n",
    "        return tensor.squeeze(0)\n",
    "\n",
    "    # make the mini-batch of images into a grid\n",
    "    nmaps = tensor.size(0)\n",
    "    xmaps = min(nrow, nmaps)\n",
    "    ymaps = int(math.ceil(float(nmaps) / xmaps))\n",
    "    height, width = int(tensor.size(2) + padding), int(tensor.size(3) + padding)\n",
    "    grid = tensor.new_full((3, height * ymaps + padding, width * xmaps + padding), pad_value)\n",
    "    k = 0\n",
    "    for y in irange(ymaps):\n",
    "        for x in irange(xmaps):\n",
    "            if k >= nmaps:\n",
    "                break\n",
    "            grid.narrow(1, y * height + padding, height - padding)\\\n",
    "                .narrow(2, x * width + padding, width - padding)\\\n",
    "                .copy_(tensor[k])\n",
    "            k = k + 1\n",
    "    return grid\n",
    "\n",
    "\n",
    "\n",
    "def from_torch_img_to_numpy(tensor, filename, nrow=8, padding=2,\n",
    "               normalize=False, range=None, scale_each=False, pad_value=0):\n",
    "    \"\"\"Save a given Tensor into an image file.\n",
    "\n",
    "    Args:\n",
    "        tensor (Tensor or list): Image to be saved. If given a mini-batch tensor,\n",
    "            saves the tensor as a grid of images by calling ``make_grid``.\n",
    "        **kwargs: Other arguments are documented in ``make_grid``.\n",
    "    \"\"\"\n",
    "    from PIL import Image\n",
    "    grid = make_grid(tensor, nrow=nrow, padding=padding, pad_value=pad_value,\n",
    "                     normalize=normalize, range=range, scale_each=scale_each)\n",
    "    # Add 0.5 after unnormalizing to [0, 255] to round to nearest integer\n",
    "    return grid.mul_(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).to('cpu', torch.uint8).numpy()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ------------- cuda -------------\n",
      "False\n",
      "Configuration: {'dataset_name': 'imagenet', 'data_with_subfolder': True, 'train_data_path': '/media/ouc/4T_B/DuAngAng/datasets/ImageNet/ILSVRC2012_img_train/', 'val_data_path': None, 'resume': None, 'batch_size': 48, 'image_shape': [256, 256, 3], 'mask_shape': [128, 128], 'mask_batch_same': True, 'max_delta_shape': [32, 32], 'margin': [0, 0], 'discounted_mask': True, 'spatial_discounting_gamma': 0.9, 'random_crop': True, 'mask_type': 'hole', 'mosaic_unit_size': 12, 'expname': 'benchmark', 'cuda': False, 'gpu_ids': [0, 1, 2], 'num_workers': 4, 'lr': 0.0001, 'beta1': 0.5, 'beta2': 0.9, 'n_critic': 5, 'niter': 500000, 'print_iter': 100, 'viz_iter': 1000, 'viz_max_out': 16, 'snapshot_save_iter': 5000, 'coarse_l1_alpha': 1.2, 'l1_loss_alpha': 1.2, 'ae_loss_alpha': 1.2, 'global_wgan_loss_alpha': 1.0, 'gan_loss_alpha': 0.001, 'wgan_gp_lambda': 10, 'netG': {'input_dim': 3, 'ngf': 32}, 'netD': {'input_dim': 3, 'ndf': 64}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/4romain/pCloud Drive/gan_inpainting_for_pose_estimation/utils/tools.py:476: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  return yaml.load(stream)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config = get_config('configs/config.yaml')\n",
    "\n",
    "# CUDA configuration\n",
    "cuda = config['cuda']\n",
    "print(' ------------- cuda -------------')\n",
    "print(cuda)\n",
    "device_ids = config['gpu_ids']\n",
    "if cuda:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = ','.join(str(i) for i in device_ids)\n",
    "    device_ids = list(range(len(device_ids)))\n",
    "    config['gpu_ids'] = device_ids\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "# Set random seed\n",
    "#if args.seed is None:\n",
    "#    args.seed = SEED#random.randint(1, 10000)\n",
    "#print(\"Random seed: {}\".format(args.seed))\n",
    "#random.seed(args.seed)\n",
    "#torch.manual_seed(args.seed)\n",
    "#if cuda:\n",
    "#    torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "print(\"Configuration: {}\".format(config))\n",
    "\n",
    "def generate(img, img_mask_path, config, model_path):\n",
    "    with torch.no_grad():   # enter no grad context\n",
    "        if img_mask_path and is_image_file(img_mask_path):\n",
    "            # Test a single masked image with a given mask\n",
    "            x = Image.fromarray(img)\n",
    "            mask = default_loader(img_mask_path)\n",
    "            x = transforms.Resize(config['image_shape'][:-1])(x)\n",
    "            x = transforms.CenterCrop(config['image_shape'][:-1])(x)\n",
    "            mask = transforms.Resize(config['image_shape'][:-1])(mask)\n",
    "            mask = transforms.CenterCrop(config['image_shape'][:-1])(mask)\n",
    "            x = transforms.ToTensor()(x)\n",
    "            mask = transforms.ToTensor()(mask)[0].unsqueeze(dim=0)\n",
    "            x = normalize(x)\n",
    "            x = x * (1. - mask)\n",
    "            x = x.unsqueeze(dim=0)\n",
    "            mask = mask.unsqueeze(dim=0)\n",
    "        elif img_mask_path:\n",
    "            raise TypeError(\"{} is not an image file.\".format(img_mask_path))\n",
    "        else:\n",
    "            # Test a single ground-truth image with a random mask\n",
    "            #ground_truth = default_loader(img_path)\n",
    "            ground_truth = img\n",
    "            ground_truth = transforms.Resize(config['image_shape'][:-1])(ground_truth)\n",
    "            ground_truth = transforms.CenterCrop(config['image_shape'][:-1])(ground_truth)\n",
    "            ground_truth = transforms.ToTensor()(ground_truth)\n",
    "            ground_truth = normalize(ground_truth)\n",
    "            ground_truth = ground_truth.unsqueeze(dim=0)\n",
    "            bboxes = random_bbox(config, batch_size=ground_truth.size(0))\n",
    "            x, mask = mask_image(ground_truth, bboxes, config)\n",
    "\n",
    "        # Set checkpoint path\n",
    "        if not model_path:\n",
    "            checkpoint_path = os.path.join('checkpoints',\n",
    "                                           config['dataset_name'],\n",
    "                                           config['mask_type'] + '_' + config['expname'])\n",
    "        else:\n",
    "            checkpoint_path = model_path\n",
    "\n",
    "        # Define the trainer\n",
    "        netG = Generator(config['netG'], cuda, device_ids)\n",
    "        # Resume weight\n",
    "        last_model_name = get_model_list(checkpoint_path, \"gen\", iteration=0)\n",
    "        \n",
    "        if cuda:\n",
    "            netG.load_state_dict(torch.load(last_model_name))\n",
    "        else:\n",
    "            netG.load_state_dict(torch.load(last_model_name, map_location='cpu'))\n",
    "                                 \n",
    "        model_iteration = int(last_model_name[-11:-3])\n",
    "        print(\"Resume from {} at iteration {}\".format(checkpoint_path, model_iteration))\n",
    "\n",
    "        if cuda:\n",
    "            netG = nn.parallel.DataParallel(netG, device_ids=device_ids)\n",
    "            x = x.cuda()\n",
    "            mask = mask.cuda()\n",
    "\n",
    "        # Inference\n",
    "        x1, x2, offset_flow = netG(x, mask)\n",
    "        inpainted_result = x2 * mask + x * (1. - mask)\n",
    "        inpainted_result =  from_torch_img_to_numpy(inpainted_result, 'output.png', padding=0, normalize=True)\n",
    "\n",
    "        return inpainted_result\n",
    "    \n",
    "def show_img(arr, img_name):\n",
    "    print(f' --------- inside {img_name} ---------')\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(arr)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_occluders(pascal_voc_root_path):\n",
    "    occluders = []\n",
    "    structuring_element = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (8, 8))\n",
    "    \n",
    "    annotation_paths = list_filepaths(os.path.join(pascal_voc_root_path, 'Annotations'))\n",
    "    for annotation_path in tqdm(annotation_paths):\n",
    "        xml_root = xml.etree.ElementTree.parse(annotation_path).getroot()\n",
    "        \n",
    "        is_segmented = (xml_root.find('segmented').text != '0')\n",
    "        \n",
    "        if not is_segmented:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        boxes = []\n",
    "        for i_obj, obj in enumerate(xml_root.findall('object')):\n",
    "            is_person = (obj.find('name').text == 'person')\n",
    "            is_difficult = (obj.find('difficult').text != '0')\n",
    "            is_truncated = (obj.find('truncated').text != '0')\n",
    "            if not is_person and not is_difficult and not is_truncated:\n",
    "                bndbox = obj.find('bndbox')\n",
    "                box = [int(bndbox.find(s).text) for s in ['xmin', 'ymin', 'xmax', 'ymax']]\n",
    "                boxes.append((i_obj, box))\n",
    "\n",
    "        if not boxes:\n",
    "            continue\n",
    "\n",
    "        im_filename = xml_root.find('filename').text\n",
    "        seg_filename = im_filename.replace('jpg', 'png')\n",
    "\n",
    "        im_path = os.path.join(pascal_voc_root_path, 'JPEGImages', im_filename)\n",
    "        seg_path = os.path.join(pascal_voc_root_path,'SegmentationObject', seg_filename)\n",
    "\n",
    "        im = np.asarray(PIL.Image.open(im_path))\n",
    "        labels = np.asarray(PIL.Image.open(seg_path))\n",
    "\n",
    "        for i_obj, (xmin, ymin, xmax, ymax) in boxes:\n",
    "            object_mask = (labels[ymin:ymax, xmin:xmax] == i_obj + 1).astype(np.uint8)*255\n",
    "            object_image = im[ymin:ymax, xmin:xmax]\n",
    "            if cv2.countNonZero(object_mask) < 500:\n",
    "                # Ignore small objects\n",
    "                continue\n",
    "\n",
    "            # Reduce the opacity of the mask along the border for smoother blending\n",
    "            eraoded = cv2.erode(object_mask, structuring_element)\n",
    "            object_mask[eroded < object_mask] = 192\n",
    "            object_with_mask = np.concatenate([object_image, object_mask[..., np.newaxis]], axis=-1)\n",
    "            \n",
    "            # Downscale for efficiency\n",
    "            object_with_mask = resize_by_factor(object_with_mask, 0.5)\n",
    "            occluders.append(object_with_mask)\n",
    "\n",
    "    return occluders\n",
    "\n",
    "\n",
    "def occlude_with_objects(im, occluders):\n",
    "    \"\"\"Returns an augmented version of `im`, containing some occluders from the Pascal VOC dataset.\"\"\"\n",
    "\n",
    "    result = im.copy()\n",
    "    width_height = np.asarray([im.shape[1], im.shape[0]])\n",
    "    occluder = occluders[8]\n",
    "    \n",
    "    im_scale_factor = min(width_height) / 256\n",
    "    random_scale_factor = np.random.uniform(0.8, 1)\n",
    "    scale_factor = random_scale_factor * im_scale_factor #* 1.2\n",
    "    occluder = resize_by_factor(occluder, scale_factor)\n",
    "    center = np.random.uniform([0+occluder.shape[0]/2,0+occluder.shape[1]/2], width_height -[occluder.shape[0]/2, occluder.shape[1]/2])\n",
    "    start_pt, end_pt = paste_over(im_src=occluder, im_dst=result, center=center)\n",
    "    \n",
    "    return result, start_pt, end_pt, center\n",
    "\n",
    "\n",
    "def paste_over(im_src, im_dst, center):\n",
    "    \"\"\"Pastes `im_src` onto `im_dst` at a specified position, with alpha blending, in place.\n",
    "\n",
    "    Locations outside the bounds of `im_dst` are handled as expected (only a part or none of\n",
    "    `im_src` becomes visible).\n",
    "\n",
    "    Args:\n",
    "        im_src: The RGBA image to be pasted onto `im_dst`. Its size can be arbitrary.\n",
    "        im_dst: The target image.\n",
    "        alpha: A float (0.0-1.0) array of the same size as `im_src` controlling the alpha blending\n",
    "            at each pixel. Large values mean more visibility for `im_src`.\n",
    "        center: coordinates in `im_dst` where the center of `im_src` should be placed.\n",
    "    \"\"\"\n",
    "\n",
    "    width_height_src = np.asarray([im_src.shape[1], im_src.shape[0]])\n",
    "    width_height_dst = np.asarray([im_dst.shape[1], im_dst.shape[0]])\n",
    "    \n",
    "    center = np.round(center).astype(np.int32)\n",
    "    raw_start_dst = center - width_height_src // 2\n",
    "    raw_end_dst = raw_start_dst + width_height_src\n",
    "\n",
    "    start_dst = np.clip(raw_start_dst, 0, width_height_dst)\n",
    "    end_dst = np.clip(raw_end_dst, 0, width_height_dst)\n",
    "    region_dst = im_dst[start_dst[1]:end_dst[1], start_dst[0]:end_dst[0]]\n",
    "\n",
    "    start_src = start_dst - raw_start_dst\n",
    "    end_src = width_height_src + (end_dst - raw_end_dst)\n",
    "    region_src = im_src[start_src[1]:end_src[1], start_src[0]:end_src[0]]\n",
    "    color_src = region_src[..., 0:3]\n",
    "    alpha = region_src[..., 3:].astype(np.float32)/255\n",
    "    im_dst[start_dst[1]:end_dst[1], start_dst[0]:end_dst[0]] = (\n",
    "            alpha * color_src + (1 - alpha) * region_dst)\n",
    "    return start_dst, end_dst\n",
    "\n",
    "\n",
    "def resize_by_factor(im, factor):\n",
    "    \"\"\"Returns a copy of `im` resized by `factor`, using bilinear interp for up and area interp\n",
    "    for downscaling.\n",
    "    \"\"\"\n",
    "    new_size = tuple(np.round(np.array([im.shape[1], im.shape[0]]) * factor).astype(int))\n",
    "    interp = cv2.INTER_LINEAR if factor > 1.0 else cv2.INTER_AREA\n",
    "    return cv2.resize(im, new_size, fx=factor, fy=factor, interpolation=interp)\n",
    "\n",
    "\n",
    "def list_filepaths(dirpath):\n",
    "    names = os.listdir(dirpath)\n",
    "    paths = [os.path.join(dirpath, name) for name in names]\n",
    "    return sorted(filter(os.path.isfile, paths))\n",
    "\n",
    "\n",
    "def show_img(arr):\n",
    "    print(' --------- occlusion ---------')\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(arr)\n",
    "    plt.show()\n",
    "    \n",
    "def show_gray_im(img):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_data_set(dataDir, dataType, annFile_full, annFile_human_pose, occluders):\n",
    "    ''' Get the data '''\n",
    "    # initialize COCO api for instance annotations\n",
    "    coco=COCO(annFile_full)\n",
    "    coco_kps=COCO(annFile_human_pose)\n",
    "\n",
    "    # get all images containing given categories, select one at random\n",
    "    catIds = coco.getCatIds(catNms=['person'])\n",
    "    imgIds = coco.getImgIds(catIds=catIds)\n",
    "\n",
    "    for img_id in imgIds:\n",
    "        # Get img from id\n",
    "        img = coco.loadImgs(img_id)[0]\n",
    "\n",
    "        annIds = coco_kps.getAnnIds(imgIds=img['id'], catIds=catIds, iscrowd=0)\n",
    "\n",
    "        if len(annIds) != 1:\n",
    "            continue\n",
    "    \n",
    "        anns = coco_kps.loadAnns(annIds)\n",
    "        if anns[0]['area'] < 3000:\n",
    "            continue\n",
    "    \n",
    "        if anns[0]['num_keypoints'] < 10:\n",
    "            continue\n",
    "    \n",
    "        arr = io.imread(img['coco_url'])\n",
    "        show_img_and_anno_from_array(arr, anns, coco_kps)\n",
    "\n",
    "        # Slice Bounding Box\n",
    "        bbox = anns[0]['bbox']\n",
    "        bbox = list(map(int, bbox))\n",
    "        bbox_arr = arr[bbox[1]:bbox[1]+bbox[3], bbox[0]:bbox[0]+bbox[2], :]\n",
    "\n",
    "        occluded_im, start_pt, end_pt, center = occlude_with_objects(bbox_arr, occluders)\n",
    "\n",
    "        # Crop the img where an occlusion appears\n",
    "        x_y_occ_mask = [end_pt[0]-start_pt[0], end_pt[1]-start_pt[1]]\n",
    "        center_arr = np.round(center).astype(np.int32)\n",
    "        c_main_img =[center_arr[0]+bbox[0], center_arr[1]+bbox[1]]\n",
    "        alpha = 5\n",
    "        x_y_mask = [c_main_img[1] - x_y_occ_mask[1] - alpha, c_main_img[0] - x_y_occ_mask[0]- alpha]\n",
    "\n",
    "        arr[bbox[1]:bbox[1]+bbox[3], bbox[0]:bbox[0]+bbox[2], :] = occluded_im\n",
    "        show_gray_im(arr)\n",
    "        #show_img_and_anno_from_array(arr, anns, coco_kps)\n",
    "\n",
    "        print(' --------------------------- CRROOOOOOOP ---------------------------')\n",
    "        max_crop_side = max(x_y_occ_mask)\n",
    "        img_for_generation = arr[x_y_mask[0]:x_y_mask[0]+max_crop_side*2 + alpha, x_y_mask[1]:x_y_mask[1]+max_crop_side*2 + alpha, :]\n",
    "        \n",
    "        print(img_for_generation.shape)\n",
    "        if (img_for_generation.shape[0] == 0) or (img_for_generation.shape[1] == 0):\n",
    "            print(' --------------------------- CONTINUE --------------------------- ')\n",
    "            continue\n",
    "        \n",
    "        gen_img = generate(img_for_generation, 'generative-inpainting-pytorch/examples/center_mask_256.png', config, '')\n",
    "        final_gen_img = cv2.resize(gen_img, (img_for_generation.shape[1],img_for_generation.shape[0]))\n",
    "        arr[x_y_mask[0]:x_y_mask[0]+max_crop_side*2 + alpha, x_y_mask[1]:x_y_mask[1]+max_crop_side*2 + alpha, :] = final_gen_img\n",
    "        show_gray_im(arr)\n",
    "        \n",
    "        print(img['id'])\n",
    "    \n",
    "        #arr[x_y_mask[0]:x_y_mask[0]+x_y_occ_mask[0]*2 + alpha, x_y_mask[1]:x_y_mask[1]+x_y_occ_mask[1]*2 + alpha, :] = 0\n",
    "        #show_gray_im(arr)\n",
    "        ##show_img_and_anno_from_array(arr, anns, coco_kps)\n",
    "\n",
    "\n",
    "def show_img_and_anno(img, anns, coco_kps):\n",
    "    plt.figure(figsize=(10,10))    \n",
    "    print(' -------------- img object -------------- ')\n",
    "    # load and display img\n",
    "    I = io.imread(img['coco_url'])\n",
    "    #plt.axis('off')\n",
    "    plt.imshow(I)\n",
    "    # load and display keypoints annotations\n",
    "    #plt.imshow(I)\n",
    "    plt.axis('off')\n",
    "    ax = plt.gca()\n",
    "    coco_kps.showAnns(anns)\n",
    "    plt.show()\n",
    "\n",
    "def show_img_and_anno_from_array(arr, anns, coco_kps):\n",
    "    plt.figure(figsize=(10,10))    \n",
    "    print(' -------------- img object -------------- ')\n",
    "    # load and display img\n",
    "    plt.imshow(arr)\n",
    "    # load and display keypoints annotations\n",
    "    #plt.imshow(I)\n",
    "    plt.axis('off')\n",
    "    ax = plt.gca()\n",
    "    coco_kps.showAnns(anns)\n",
    "    plt.show()\n",
    "    \n",
    "def show_gray_im(img):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(img, 'gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#occluders = load_occluders('synthetic-occlusion/VOCdevkit/VOC2012')\n",
    "import pickle\n",
    "\n",
    "#with open('pre_process_folder/occluders.pkl', 'wb') as f:\n",
    "#    pickle.dump(occluders, f)\n",
    "    \n",
    "with open('pre_process_folder/occluders.pkl', 'rb') as f:\n",
    "    occluders = pickle.load(f)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_data_set(dataDir, dataType, annFile_full, annFile_human_pose, occluders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
